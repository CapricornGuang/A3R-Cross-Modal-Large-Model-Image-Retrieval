# 跨膜态大模型图像检索比赛第6名解决方案
![PyTorch 1.12.1](https://img.shields.io/badge/PyTorch-1.12.1-green?style=plastic)
![OpenClipTorch 2.17.1](https://img.shields.io/badge/OpenClipTorch-2.17.1-orange?style=plastic)
![CVPR 2023](https://img.shields.io/badge/CVPR-2023-red?style=plastic)


## 赛题背景
交通场景中高性能的图像检索能力对于交通执法、治安治理具有十分重要的作用，传统的图像检索方式通常使用先对图像进行属性识别再通过与期望属性的对比实现检索能力。随着多模态大模型技术的发展，文本与图像的表征统一和模态转换已有广泛应用，使用该能力可以进一步提升图像检索的精度和灵活性。

## 赛题任务
本赛道旨在提升交通场景中文本图像检索的精度。因此我们将多种公开数据集以及网络数据中的交通参与者图像进行了文本描述标注从而构建了多对多的图像-文本对，选手可以在此基础上进行多模态技术的研究工作，提升文本检索图像的精度。

# 复现
## 赛制审核
1. 可一键复现的Pytorch算法代码：notebook-reproduce.ipynb提供了用于复现的一键运行Jupyter Notebook, notebook-quick-review.ipynb提供了用于快速得到最优结果文件的Jupyter Notebook
2. 提交模型文件对应的checkpoint：日志保存在best-result-review中，模型需要另外下载
   下载链接：https://pan.baidu.com/s/17P6nzWl9PnVH42DFQsCd_w 提取码：067e
3. 代码内容说明：在notebook-reproduce.ipynb与notebook-reproduce.ipynb中提供了详细说明
4. 模型构建思路
   （1）完整算法结构框图、思路步骤详述、代码组织结构介绍：见如下介绍
   （2）数据增强/清洗策略：见如下介绍
   （3）调参优化策略（若多次迭代，还需说明迭代的具体策略）：见如下介绍
   （4）训练脚本/代码，最好包含训练一个epoch的运行日志: 在"可一键复现的Pytorch算法代码, notebook-reproduce.ipynb"部分提供
   （5）测试脚本/代码，必须包含评估得到最终精度的运行日志: 在"可一键复现的Pytorch算法代码, notebook-quick-review.ipynb"部分提供
## 代码结构
The tree below illustrates the organization of this project.
```bash
├── data
│   ├── augmented*.txt #数据增强文件
│   ├── car_attrbute*.json #汽车属性文件
│   ├── dataset #数据集（ImageRetrival，提供在百度盘需要自己下载）
├── best-result-review #与log一致，但该文件夹单独保存了最优提交的运行记录
│   │   ├──out.log
│   │   ├──params.txt
│   │   ├──checkpoints #需要自行下载
│   │   ├──tensorboard
├── log
│   ├── * #日志文件夹（每次训练都会生成一个专有的文件夹）
│   │   ├──out.log #输出训练日志
│   │   ├──params.txt #超参数
│   │   ├──checkpoints #模型权重
│   │   ├──tensorboard #cd 到 *目录后运行 tensorboard --logdir = ./tensorboard --host localhost --port 20421 会在localhost:20421打开当前训练的tensorboard
├── script #运行脚本，建议在notebook-reproduce.ipynb中看
├── src 
│   ├── infer
│   │   ├──merge_json.py
│   │   ├──open_clip_infer.py #推理代码
│   ├── preprocess
│   │   ├──parse_attr.py #得到汽车属性文件
│   │   ├──parse_augment.py #解析数据增强后的文件格式为CLIP训练需要的格式
│   │   ├──parse_split.py #解析*_person_label.txt, *_car_label.txt(分离后的data/datasets/*_label.txt)的文件格式为CLIP训练需要的格式
│   ├── train
│   │   ├──* #参考https://github.com/mlfoundations/open_clip
│   │   ├──data #存放了open_clip用于推理的csv文件
│   │   ├──training
│   │   │   ├──*
│   │   │   ├──params.py #script/run_model.sh中对于各个参数的说明
│   │   │   ├──main.py #模型训练时运行的主文件 
```

## 模型设计
本比赛主要利用CLIP方法进行多模态对比学习训练, 整体的训练思路如下：
<p align="center">
<img src=".\.img/framework.png" height = "240" alt="" align=center />
<br><br>
<b>图1.整体思路</b>
</p>

## 优化
以下描述的均是能够提高A榜单得分的策略
### 模型选择
大模型内部存在大量的隐式知识，模型的网络通路也具备更强的鲁棒性。在本项目发现大模型对于光照具有很好的鲁棒性，具备很好的白平衡能力
选取ViT-G-14大模型作为主干网络
### 数据