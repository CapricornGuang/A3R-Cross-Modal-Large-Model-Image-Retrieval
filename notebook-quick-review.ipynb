{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec201aa4",
   "metadata": {},
   "source": [
    "# 使用最优的模型获得最佳的结果（Best History Version）\n",
    "- 将最佳模型的epoch5.pt文件下载到本地，替换--pt_path参数即可\n",
    "- 该模型在加载到显存后约10G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "708893d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading...\n",
      "Compose(\n",
      "    Lambda()\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x7f640ea9f200>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "data loading...\n",
      "100%|████████████████████████████████| 10000/10000 [00:00<00:00, 2759410.53it/s]\n",
      "100%|████████████████████████████████| 10000/10000 [00:00<00:00, 3583037.76it/s]\n",
      "query text number: 10000\n",
      "key image number: 17611\n",
      "start loading text features\n",
      "100%|█████████████████████████████████████████████| 5/5 [01:27<00:00, 17.45s/it]\n",
      "start loading image features\n",
      "100%|███████████████████████████████████████████| 89/89 [30:57<00:00, 20.87s/it]\n",
      "similarity_argsort.shape (10000, 17611)\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0; python src/infer/open_clip_infer.py --model_name ViT-bigG-14 --pt_path /data1/code/yyg/image_retrival/yyg/open_clip_G_real/open_clip-G/open_clip/logs/epoch_5.pt --image_root data/datasets/test/test_images/ --text_path data/datasets/test/test_person_text.txt --run_name open_clip_person_infer --text_steps 2000 --image_batch 200 --topk 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9b29eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loading...\n",
      "Compose(\n",
      "    Lambda()\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_to_rgb at 0x7fa2bac4f200>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      "data loading...\n",
      "100%|██████████████████████████████████| 7611/7611 [00:00<00:00, 1210344.94it/s]\n",
      "100%|██████████████████████████████████| 7611/7611 [00:00<00:00, 4257515.04it/s]\n",
      "query text number: 7611\n",
      "key image number: 17611\n",
      "start loading text features\n",
      "100%|█████████████████████████████████████████████| 4/4 [01:05<00:00, 16.40s/it]\n",
      "start loading image features\n",
      "100%|███████████████████████████████████████████| 89/89 [30:13<00:00, 20.38s/it]\n",
      "similarity_argsort.shape (7611, 17611)\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1; python src/infer/open_clip_infer.py --model_name ViT-bigG-14 --pt_path /data1/code/yyg/image_retrival/yyg/open_clip_G_real/open_clip-G/open_clip/logs/epoch_5.pt --image_root data/datasets/test/test_images/ --text_path data/datasets/test/test_car_text.txt --run_name open_clip_car_infer --text_steps 2000 --image_batch 200 --topk 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85347550",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python src/infer/merge_json.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
